{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMbvKS6Vc8pMMreT+FsM8qO"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install datasets"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ug3TYxzhR6Na","executionInfo":{"status":"ok","timestamp":1718847300752,"user_tz":-120,"elapsed":13183,"user":{"displayName":"Lorenzo Vaiani","userId":"03185110435146403344"}},"outputId":"aac449bf-a617-46b0-9608-4d7f0353da7d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.20.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.14.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (16.1.0)\n","Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n","Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n","Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n","Requirement already satisfied: fsspec[http]<=2024.5.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n","Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.3)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.6.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"]}]},{"cell_type":"code","source":["import numpy as np\n","import torch\n","import pickle\n","from tqdm import tqdm\n","from torch.utils.data import Dataset, DataLoader\n","import logging\n","logging.basicConfig(level=logging.ERROR)\n","from transformers import AdamW\n","from tqdm import tqdm\n","from datasets import load_metric\n","import numpy as np\n","import pprint\n","import copy\n","from torch.nn import TransformerEncoder, TransformerEncoderLayer"],"metadata":{"id":"Lsvsg91FPoF-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Dataset Class"],"metadata":{"id":"pasetHTUQS1X"}},{"cell_type":"code","source":["class CustomDataset(Dataset):\n","    def __init__(self, doc_info, padding_len=100):\n","        \"\"\"\n","        Args:\n","            annotations (List[List]): List of lists containing the word-level annotations (words, labels, boxes).\n","            image_dir (string): Directory with all the document images.\n","            processor (LayoutLMv2Processor): Processor to prepare the text + image.\n","        \"\"\"\n","        self.doc_info = doc_info\n","        self.padding_len = padding_len\n","\n","    def __len__(self):\n","        return len(self.doc_info)\n","\n","    def __getitem__(self, idx):\n","        doc_info = self.doc_info[list(self.doc_info.keys())[idx]]\n","        target_id = []\n","        visual_feat = doc_info[\"visual_list\"]\n","        bert_cls = doc_info[\"bert_cls\"]\n","        for i in range(len(doc_info[\"form\"])):\n","            target_id.append(doc_info[\"form\"][i][\"label\"])\n","        target_id = [label_dict[l] for l in target_id]\n","\n","        visual_feat = copy.deepcopy(visual_feat)\n","        bert_cls = copy.deepcopy(bert_cls)\n","\n","        object_mask = []\n","        if len(visual_feat) >= self.padding_len:\n","            visual_feat = visual_feat[:self.padding_len]\n","            bert_cls = bert_cls[:self.padding_len]\n","            object_mask = [1]*self.padding_len\n","\n","            target_id = target_id[:self.padding_len]\n","        else:\n","            size = len(visual_feat)\n","            visual_feat.extend([[0.0]*2048]*(self.padding_len-len(visual_feat)))\n","            bert_cls.extend([[0.0]*768]*(self.padding_len-len(bert_cls)))\n","            object_mask = [1]*size+[0.0]*(self.padding_len-size)\n","            target_id.extend([-100]*(self.padding_len-len(target_id)))\n","\n","        return {\n","            'visual_feat': torch.tensor(visual_feat, dtype=torch.float),\n","            'bert_cls': torch.tensor(bert_cls, dtype=torch.float),\n","            'target': torch.tensor(target_id, dtype=torch.float),\n","            'object_mask':torch.tensor(object_mask, dtype=torch.float),\n","        }"],"metadata":{"id":"jOmfb-7xPqcF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Model Class\n","It is a standard transformer-based architecture implemented using TransformerEncoderLayers from PyTorch\n","Token have a size equal to the sum of textual and visual embeddings, i.e., 768 + 2048 = 2816"],"metadata":{"id":"3OrXTvxNQVjR"}},{"cell_type":"code","source":["class New_model(torch.nn.Module):\n","    def __init__(self):\n","        super(New_model, self).__init__()\n","\n","        self.encoder_layer = TransformerEncoderLayer(d_model=2816, nhead=16)\n","        self.encoder = TransformerEncoder(self.encoder_layer, num_layers=6)\n","        self.pre_classifier = torch.nn.Linear(2816, 2816)\n","        self.dropout = torch.nn.Dropout(0.1)\n","        self.classifier = torch.nn.Linear(2816, 4)\n","\n","    def forward(self, textual_embed, visual_embed, attention_mask):\n","\n","        embed = torch.cat((textual_embed, visual_embed), dim=2)\n","        attention_mask = attention_mask.transpose(0,1)\n","\n","        output_1 = self.encoder(embed, src_key_padding_mask=attention_mask)\n","        output = self.pre_classifier(output_1)\n","        output = torch.nn.Tanh()(output)\n","        output = self.dropout(output)\n","        output = self.classifier(output)\n","        return output"],"metadata":{"id":"3E7jA5RRPv1a"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Compute metric function definition"],"metadata":{"id":"1gSSv-qFQxWq"}},{"cell_type":"code","source":["def compute_metrics(p):\n","    predictions, labels = p\n","    predictions = np.argmax(predictions, axis=2)\n","    # Remove ignored index (special tokens)\n","    true_predictions = [\n","        [label_list[int(p)] for (p, l) in zip(prediction, label) if l != -100]\n","        for prediction, label in zip(predictions, labels)\n","    ]\n","    true_labels = [\n","        [label_list[int(l)] for (p, l) in zip(prediction, label) if l != -100]\n","        for prediction, label in zip(predictions, labels)\n","    ]\n","    results = metric.compute(predictions=true_predictions, references=true_labels)\n","    if return_entity_level_metrics:\n","        # Unpack nested dictionaries\n","        final_results = {}\n","        for key, value in results.items():\n","            if isinstance(value, dict):\n","                for n, v in value.items():\n","                    final_results[f\"{key}_{n}\"] = v\n","            else:\n","                final_results[key] = value\n","        return final_results\n","    else:\n","        return results\n"],"metadata":{"id":"x2MbTSNZPzGq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Main\n","\n","Load textual and visual precomputed features"],"metadata":{"id":"nbnObPNMP29f"}},{"cell_type":"code","source":["with open(\"data/FUNSD/dataset/training_data/all_annotations_visual.pickle\", 'rb') as f:\n","    train_data_visual = pickle.load(f)\n","\n","with open(\"data/FUNSD/dataset/training_data/all_annotations_textual.pickle\", 'rb') as f:\n","    train_data_textual = pickle.load(f)\n","\n","with open(\"data/FUNSD/dataset/testing_data/all_annotations_visual.pickle\", 'rb') as f:\n","    test_data_visual = pickle.load(f)\n","\n","with open(\"data/FUNSD/dataset/testing_data/all_annotations_textual.pickle\", 'rb') as f:\n","    test_data_textual = pickle.load(f)\n"],"metadata":{"id":"6QWAmjomP7J6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["label_dict = {}\n","label_id = 0\n","for doc in train_data_textual:\n","    for i in range(len(train_data_textual[doc][\"form\"])):\n","        if train_data_textual[doc][\"form\"][i][\"label\"] not in label_dict:\n","            label_dict[train_data_textual[doc][\"form\"][i][\"label\"]] = label_id\n","            label_id += 1\n","\n","for doc in train_data_textual:\n","    train_data_textual[doc][\"visual_list\"] = train_data_visual[doc][\"visual_list\"]\n","\n","for doc in test_data_textual:\n","    test_data_textual[doc][\"visual_list\"] = test_data_visual[doc][\"visual_list\"]"],"metadata":{"id":"bfo9B45wQAh8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Define model, loss and optimizer and create dataset objects"],"metadata":{"id":"v7REHls-Q76s"}},{"cell_type":"code","source":["model = New_model()\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model.to(device)\n","\n","metric = load_metric(\"seqeval\")\n","return_entity_level_metrics = True\n","label_list = list(label_dict.keys())\n","\n","train_dataset = CustomDataset(doc_info=train_data_textual)\n","test_dataset = CustomDataset(doc_info=test_data_textual)\n","train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n","test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n","\n","optimizer = AdamW(model.parameters(), lr=2e-5)\n","loss_function = torch.nn.CrossEntropyLoss()\n","model.train()"],"metadata":{"id":"pNsauoLwQEYA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Train and Validation functions"],"metadata":{"id":"UTLNE9SbRF2Y"}},{"cell_type":"code","source":["def train(num_train_epochs):\n","    for _ in range(num_train_epochs):\n","        total_loss = 0\n","        for data in tqdm(train_dataloader):\n","            # get the inputs;\n","            labels = data['target'].to(device, dtype = torch.long)\n","            visual_feats = data['visual_feat'].to(device, dtype = torch.float)\n","            bert_cls = data['bert_cls'].to(device, dtype = torch.float)\n","            object_mask = data['object_mask'].to(device, dtype = torch.float)\n","\n","            optimizer.zero_grad()\n","            outputs = model(bert_cls,visual_feats,object_mask)\n","            # Change the number of categories\n","            loss = loss_function(outputs.view(-1, 4), labels.view(-1)) #\n","            loss.backward()\n","            total_loss += loss.item()\n","            optimizer.step()\n","        print(\"Train Loss:\", total_loss/len(train_dataloader))\n"],"metadata":{"id":"UY8LpYgSQIVq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def eval(test_dataloader):\n","    preds_val = None\n","    out_label_ids = None\n","    model.eval()\n","    total_loss = 0\n","    for data in tqdm(test_dataloader):\n","        with torch.no_grad():\n","            # get the inputs;\n","            labels = data['target'].to(device, dtype = torch.long)\n","            visual_feats = data['visual_feat'].to(device, dtype = torch.float)\n","            bert_cls = data['bert_cls'].to(device, dtype = torch.float)\n","            object_mask = data['object_mask'].to(device, dtype = torch.float)\n","\n","            outputs = model(bert_cls,visual_feats,object_mask)\n","            loss = loss_function(outputs.view(-1, 4), labels.view(-1))\n","            total_loss += loss.item()\n","            if preds_val is None:\n","                preds_val = outputs.detach().cpu().numpy()\n","                out_label_ids = data[\"target\"].detach().cpu().numpy()\n","            else:\n","                preds_val = np.append(preds_val, outputs.detach().cpu().numpy(), axis=0)\n","                out_label_ids = np.append(out_label_ids, data[\"target\"].detach().cpu().numpy(), axis=0)\n","    print(\"Val Loss:\", total_loss/len(test_dataloader))\n","    pprint.pprint(compute_metrics((preds_val, out_label_ids)))\n","    return compute_metrics((preds_val, out_label_ids))"],"metadata":{"id":"45s6t6ghQKny"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Train Loop\n","\n","Run training for 30 epochs, validating the model every epoch. Everytime a new best model is found, the checkpoint is saved. At the end of trainig loop best performing epoch and associated evaluation results are printed"],"metadata":{"id":"aO9EYWOsRNc8"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"xt7LN--_PkkF"},"outputs":[],"source":["current_f1 = 0\n","best_epoch = 0\n","optimizer = AdamW(model.parameters(), lr=1e-5)\n","\n","for epoch in range(30):\n","    print(\"Epoch:\", epoch+1)\n","    train(1)\n","    val_result = eval(test_dataloader)\n","    if val_result['overall_f1'] > current_f1:\n","        current_f1 = val_result['overall_f1']\n","        best_epoch = epoch\n","        best_val_result = val_result\n","        torch.save(model, 'results/vanilla-transformer_funsd.pth')\n","\n","print(\"Best Epoch:\", best_epoch+1)\n","pprint.pprint(best_val_result)\n"]}]}