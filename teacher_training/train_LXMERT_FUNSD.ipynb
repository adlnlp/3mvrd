{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMxrR/QSJV6IK84CxtbFZpV"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install datasets"],"metadata":{"id":"rb5r2hU-bSs1"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rFkpHYstUGr_"},"outputs":[],"source":["import numpy as np\n","import torch\n","import pickle\n","from tqdm import tqdm\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import LxmertTokenizer, LxmertModel\n","import logging\n","logging.basicConfig(level=logging.ERROR)\n","import math\n","from transformers import AdamW\n","from tqdm import tqdm\n","from datasets import load_metric\n","import numpy as np\n","import pprint\n","import copy\n","from PIL import Image"]},{"cell_type":"markdown","source":["LXMERT utility functions: 1D-positional encoding and Bounding Box normaliation"],"metadata":{"id":"zU5h-H_pbV7y"}},{"cell_type":"code","source":["def positionalencoding1d(d_model, feature_list):\n","    \"\"\"\n","    :param d_model: dimension of the model\n","    :param feature_list: length of positions\n","    :return: length*d_model position matrix\n","    \"\"\"\n","    if d_model % 2 != 0:\n","        raise ValueError(\"Cannot use sin/cos positional encoding with \"\n","                         \"odd dim (got dim={:d})\".format(d_model))\n","    pe = torch.zeros(1, d_model)\n","    feats = torch.tensor(feature_list)\n","    div_term = torch.exp((torch.arange(0, d_model, 2, dtype=torch.float) *\n","                         -(math.log(10000.0) / d_model)))\n","    pe[:, 0::2] = torch.sin(feats.float() * div_term)\n","    pe[:, 1::2] = torch.cos(feats.float() * div_term)\n","    pe = np.array(pe.tolist())\n","    return pe\n","\n","def normalize_bbox(bbox, width, height):\n","    x1 = bbox[0]\n","    y1 = bbox[1]\n","    x2 = bbox[2]\n","    y2 = bbox[3]\n","    return [x1/width, y1/height,abs(x2-x1)/width,abs(y2-y1)/height]"],"metadata":{"id":"lK4OwToPbPe0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Dataset Class"],"metadata":{"id":"72s8YnbEbf7x"}},{"cell_type":"code","source":["class CustomDataset(Dataset):\n","\n","    def __init__(self, doc_info, tokenizer, positional_encoding, padding_len=100, split=\"train\"):\n","        \"\"\"\n","        Args:\n","            annotations (List[List]): List of lists containing the word-level annotations (words, labels, boxes).\n","            image_dir (string): Directory with all the document images.\n","            processor (LayoutLMv2Processor): Processor to prepare the text + image.\n","        \"\"\"\n","        self.tokenizer = tokenizer\n","        self.doc_info = doc_info\n","        self.positional_encoding = positional_encoding\n","        self.padding_len = padding_len\n","        self.split = split\n","\n","    def __len__(self):\n","        return len(self.doc_info)\n","\n","    def __getitem__(self, idx):\n","        key = list(self.doc_info.keys())[idx]\n","        doc_info = self.doc_info[key]\n","        image_path = \"data/FUNSD/dataset/\"+self.split+\"ing_data/images/\"+key+\".png\"\n","        image = Image.open(image_path)\n","        width, height = image.size\n","\n","        texts = []\n","        target_id = []\n","        boxes = []\n","        for i in range(len(doc_info[\"form\"])):\n","            texts.append(doc_info[\"form\"][i][\"text\"])\n","            target_id.append(doc_info[\"form\"][i][\"label\"])\n","            boxes.append(copy.deepcopy(doc_info[\"form\"][i][\"box\"]))\n","        target_id = [label_dict[l] for l in target_id]\n","        text = \" \".join(texts)\n","        # Get Fine-Grained Level Information\n","        inputs = self.tokenizer.encode_plus(\n","            text,\n","            None,\n","            add_special_tokens=True,\n","            max_length=512,\n","            pad_to_max_length=True,\n","            return_token_type_ids=True\n","        )\n","        ids = inputs['input_ids']\n","        mask = inputs['attention_mask']\n","        token_type_ids = inputs[\"token_type_ids\"]\n","\n","        object_mask = []\n","        visual_feat = copy.deepcopy(doc_info[\"visual_list\"])\n","        norm_bbox = [normalize_bbox(bbox, width, height) for bbox in boxes]\n","\n","        if len(visual_feat) >= self.padding_len:\n","            visual_feat = visual_feat[:self.padding_len]\n","            norm_bbox = norm_bbox[:self.padding_len]\n","            object_mask = [1]*self.padding_len\n","            target_id = target_id[:self.padding_len]\n","        else:\n","            size = len(visual_feat)\n","            visual_feat.extend([[0.0]*2048]*(self.padding_len-len(visual_feat)))\n","            norm_bbox.extend([[0.0]*4]*(self.padding_len-len(norm_bbox)))\n","            object_mask = [1]*size+[0.0]*(self.padding_len-size)\n","            target_id.extend([-100]*(self.padding_len-len(target_id)))\n","\n","        return {\n","            'ids': torch.tensor(ids, dtype=torch.long), #Key information input ids\n","            'mask': torch.tensor(mask, dtype=torch.float), # Key information masks\n","            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long), # Key information token type ids\n","            'visual_feat': torch.tensor(visual_feat, dtype=torch.float),\n","            'target': torch.tensor(target_id, dtype=torch.float),\n","            'positional_encoding': torch.tensor(self.positional_encoding, dtype = torch.float),\n","            'object_mask':torch.tensor(object_mask, dtype=torch.float),\n","            'norm_bbox':torch.tensor(norm_bbox, dtype=torch.float)\n","        }"],"metadata":{"id":"Cn5EG0bGbK9c"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Model Class"],"metadata":{"id":"424F3VEzbh06"}},{"cell_type":"code","source":["class New_model(torch.nn.Module):\n","    def __init__(self):\n","        super(New_model, self).__init__()\n","        self.l1 = LxmertModel.from_pretrained(\"unc-nlp/lxmert-base-uncased\",output_hidden_states = True)\n","        self.pre_classifier = torch.nn.Linear(768, 768)\n","        self.dropout = torch.nn.Dropout(0.1)\n","        self.classifier = torch.nn.Linear(768, 4)\n","\n","    def forward(self, ids, mask, token_type_ids, visual_feat,attention_mask,norm_bbox):\n","        visual_attention_mask = attention_mask.to(device, dtype = torch.float)\n","        output_1 = self.l1(input_ids=ids,\n","                           attention_mask=mask,\n","                           token_type_ids=token_type_ids,\n","                           visual_feats =visual_feat,\n","                           visual_pos = norm_bbox,\n","                           visual_attention_mask=visual_attention_mask)\n","        visual_feat = output_1.vision_hidden_states[-1]\n","        output = self.pre_classifier(visual_feat)\n","        output = torch.nn.Tanh()(output)\n","        output = self.dropout(output)\n","        output = self.classifier(output)\n","        return output"],"metadata":{"id":"xNx2sLgObHn0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Compute metric function"],"metadata":{"id":"ExJSYNFYblzI"}},{"cell_type":"code","source":["def compute_metrics(p):\n","    predictions, labels = p\n","    predictions = np.argmax(predictions, axis=2)\n","    # Remove ignored index (special tokens)\n","    true_predictions = [\n","        [label_list[int(p)] for (p, l) in zip(prediction, label) if l != -100]\n","        for prediction, label in zip(predictions, labels)\n","    ]\n","    true_labels = [\n","        [label_list[int(l)] for (p, l) in zip(prediction, label) if l != -100]\n","        for prediction, label in zip(predictions, labels)\n","    ]\n","    results = metric.compute(predictions=true_predictions, references=true_labels)\n","    if return_entity_level_metrics:\n","        # Unpack nested dictionaries\n","        final_results = {}\n","        for key, value in results.items():\n","            if isinstance(value, dict):\n","                for n, v in value.items():\n","                    final_results[f\"{key}_{n}\"] = v\n","            else:\n","                final_results[key] = value\n","        return final_results\n","    else:\n","        return results"],"metadata":{"id":"jkWPa6zdbFrb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Main\n","\n","Load visual precomputed features. Precomputed textual features are not needed because ViasualBERT uses its own tokenizer."],"metadata":{"id":"7sTsxxjkbnvo"}},{"cell_type":"code","source":["with open(\"data/FUNSD/dataset/training_data/all_annotations_visual.pickle\", 'rb') as f:\n","    train_data = pickle.load(f)\n","\n","with open(\"data/FUNSD/dataset/testing_data/all_annotations_visual.pickle\", 'rb') as f:\n","    test_data = pickle.load(f)\n"],"metadata":{"id":"u2lv17R6bBAr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Define model, loss and optimizer and create dataset objects"],"metadata":{"id":"2KrBBaIhbx5S"}},{"cell_type":"code","source":["label_dict = {}\n","label_id = 0\n","for doc in train_data:\n","    for i in range(len(train_data[doc][\"form\"])):\n","        if train_data[doc][\"form\"][i][\"label\"] not in label_dict:\n","            label_dict[train_data[doc][\"form\"][i][\"label\"]] = label_id\n","            label_id += 1\n","\n","tokenizer = LxmertTokenizer.from_pretrained('unc-nlp/lxmert-base-uncased')\n","model = New_model()\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model.to(device)\n","\n","metric = load_metric(\"seqeval\")\n","return_entity_level_metrics = True\n","label_list = list(label_dict.keys())\n","\n","positional_encoding = []\n","for i in range(100):\n","    positional_encoding.append(positionalencoding1d(768,i)[0])\n","positional_encoding = np.array(positional_encoding)\n","\n","train_dataset = CustomDataset(doc_info=train_data, tokenizer=tokenizer, positional_encoding=positional_encoding, split=\"train\")\n","test_dataset = CustomDataset(doc_info=test_data, tokenizer=tokenizer, positional_encoding=positional_encoding, split=\"test\")\n","train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n","test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n","\n","optimizer = AdamW(model.parameters(), lr=2e-5)\n","loss_function = torch.nn.CrossEntropyLoss()\n","model.train()"],"metadata":{"id":"ROCE0w50a_TG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Train and Validation functions\n"],"metadata":{"id":"5Q03wUl3b1T0"}},{"cell_type":"code","source":["def train(num_train_epochs):\n","    for _ in range(num_train_epochs):\n","        total_loss = 0\n","        for data in tqdm(train_dataloader):\n","            # get the inputs;\n","            ids = data['ids'].to(device, dtype = torch.long)\n","            mask = data['mask'].to(device, dtype = torch.float)\n","            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n","            labels = data['target'].to(device, dtype = torch.long)\n","            visual_feats = data['visual_feat'].to(device, dtype = torch.float)\n","            norm_bbox = data['norm_bbox'].to(device, dtype = torch.float)\n","            object_mask = data['object_mask'].to(device, dtype = torch.float)\n","\n","            optimizer.zero_grad()\n","            outputs = model(ids, mask, token_type_ids,visual_feats,object_mask,norm_bbox)\n","            # Change the number of categories\n","            loss = loss_function(outputs.view(-1, 4), labels.view(-1)) #\n","            loss.backward()\n","            total_loss += loss.item()\n","            optimizer.step()\n","        print(\"Train Loss:\", total_loss/len(train_dataloader))\n","\n","\n"],"metadata":{"id":"VrXJYRxka8m8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def eval(test_dataloader):\n","    preds_val = None\n","    out_label_ids = None\n","    model.eval()\n","    total_loss = 0\n","    for data in tqdm(test_dataloader):\n","        with torch.no_grad():\n","            # get the inputs;\n","            ids = data['ids'].to(device, dtype = torch.long)\n","            mask = data['mask'].to(device, dtype = torch.float)\n","            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n","            visual_feats = data['visual_feat'].to(device, dtype = torch.float)\n","            norm_bbox = data['norm_bbox'].to(device, dtype = torch.float)\n","            object_mask = data['object_mask'].to(device, dtype = torch.float)\n","            labels = data['target'].to(device, dtype = torch.long)\n","\n","            optimizer.zero_grad()\n","            outputs = model(ids, mask, token_type_ids, visual_feats, object_mask,norm_bbox)\n","            loss = loss_function(outputs.view(-1, 4), labels.view(-1))\n","            total_loss += loss.item()\n","\n","            if preds_val is None:\n","                preds_val = outputs.detach().cpu().numpy()\n","                out_label_ids = data[\"target\"].detach().cpu().numpy()\n","            else:\n","                preds_val = np.append(preds_val, outputs.detach().cpu().numpy(), axis=0)\n","                out_label_ids = np.append(out_label_ids, data[\"target\"].detach().cpu().numpy(), axis=0)\n","\n","    print(\"Val Loss:\", total_loss/len(test_dataloader))\n","    pprint.pprint(compute_metrics((preds_val, out_label_ids)))\n","    return compute_metrics((preds_val, out_label_ids))"],"metadata":{"id":"F9XFzFwsa66J"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Train Loop\n","\n","Run training for 30 epochs, validating the model every epoch. Everytime a new best model is found, the checkpoint is saved. At the end of trainig loop best performing epoch and associated evaluation results are printed"],"metadata":{"id":"KpttNDJ8b2oY"}},{"cell_type":"code","source":["current_f1 = 0\n","best_epoch = 0\n","optimizer = AdamW(model.parameters(), lr=1e-5)\n","\n","for epoch in range(30):\n","    print(\"Epoch:\", epoch+1)\n","    train(1)\n","    val_result = eval(test_dataloader)\n","    if val_result['overall_f1'] > current_f1:\n","        current_f1 = val_result['overall_f1']\n","        best_epoch = epoch\n","        best_val_result = val_result\n","        torch.save(model, 'results/lxmert_funsd.pth')\n","\n","print(\"Best Epoch:\", best_epoch+1)\n","pprint.pprint(best_val_result)"],"metadata":{"id":"dAB4Wl_ga5E4"},"execution_count":null,"outputs":[]}]}