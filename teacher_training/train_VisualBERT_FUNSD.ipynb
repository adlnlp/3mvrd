{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP0B0C7h9+7idbKh2JBATKw"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install datasets"],"metadata":{"id":"6CY3C1WOSaJJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import torch\n","import pickle\n","from tqdm import tqdm\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import  VisualBertModel, BertTokenizer\n","import logging\n","logging.basicConfig(level=logging.ERROR)\n","from transformers import AdamW\n","from tqdm import tqdm\n","from datasets import load_metric\n","import numpy as np\n","import pprint\n","import copy"],"metadata":{"id":"gC0HkjIcSdU6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Dataset Class"],"metadata":{"id":"Xbsaz6I6S-yD"}},{"cell_type":"code","source":["class CustomDataset(Dataset):\n","    def __init__(self, doc_info, tokenizer, padding_len=100):\n","        \"\"\"\n","        Args:\n","            annotations (List[List]): List of lists containing the word-level annotations (words, labels, boxes).\n","            image_dir (string): Directory with all the document images.\n","            processor (LayoutLMv2Processor): Processor to prepare the text + image.\n","        \"\"\"\n","        self.tokenizer = tokenizer\n","        self.doc_info = doc_info\n","        self.padding_len = padding_len\n","\n","    def __len__(self):\n","        return len(self.doc_info)\n","\n","    def __getitem__(self, idx):\n","        doc_info = self.doc_info[list(self.doc_info.keys())[idx]]\n","        texts = []\n","        target_id = []\n","        visual_feat = doc_info[\"visual_list\"]\n","        for i in range(len(doc_info[\"form\"])):\n","            texts.append(doc_info[\"form\"][i][\"text\"])\n","            target_id.append(doc_info[\"form\"][i][\"label\"])\n","        target_id = [label_dict[l] for l in target_id]\n","        text = \" \".join(texts)\n","        # Get Fine-Grained Level Information\n","        inputs = self.tokenizer.encode_plus(\n","            text,\n","            None,\n","            add_special_tokens=True,\n","            max_length=512,\n","            pad_to_max_length=True,\n","            return_token_type_ids=True\n","        )\n","        ids = inputs['input_ids']\n","        mask = inputs['attention_mask']\n","        token_type_ids = inputs[\"token_type_ids\"]\n","        visual_feat = copy.deepcopy(visual_feat)\n","\n","        object_mask = []\n","        if len(visual_feat) >= self.padding_len:\n","            visual_feat = visual_feat[:self.padding_len]\n","            object_mask = [1]*self.padding_len\n","            target_id = target_id[:self.padding_len]\n","        else:\n","            size = len(visual_feat)\n","            visual_feat.extend([[0.0]*2048]*(self.padding_len-len(visual_feat)))\n","            object_mask = [1]*size+[0.0]*(self.padding_len-size)\n","            target_id.extend([-100]*(self.padding_len-len(target_id)))\n","\n","        return {\n","            'ids': torch.tensor(ids, dtype=torch.long), #Key information input ids\n","            'mask': torch.tensor(mask, dtype=torch.float), # Key information masks\n","            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long), # Key information token type ids\n","            'visual_feat': torch.tensor(visual_feat, dtype=torch.float),\n","            'target': torch.tensor(target_id, dtype=torch.float),\n","            'object_mask':torch.tensor(object_mask, dtype=torch.float),\n","        }"],"metadata":{"id":"GvOqV3WtShn-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Model Class\n","\n","It uses a pretrained VisualBERT model, considering all the output embeddings corresponding to visual tokens (from token 512 onwards) to perform the final classification."],"metadata":{"id":"_CYHEFqPTAqs"}},{"cell_type":"code","source":["class New_model(torch.nn.Module):\n","    def __init__(self):\n","        super(New_model, self).__init__()\n","\n","        self.l1 = VisualBertModel.from_pretrained(\"uclanlp/visualbert-vqa-coco-pre\",output_hidden_states=True)\n","        self.pre_classifier = torch.nn.Linear(768, 768)\n","        self.dropout = torch.nn.Dropout(0.1)\n","        self.classifier = torch.nn.Linear(768, 4)\n","\n","    def forward(self, ids, mask, token_type_ids, visual_feat, attention_mask):\n","        visual_token_type_ids = torch.ones(visual_feat.shape[:-1], dtype=torch.long).to(device, dtype = torch.long)\n","        visual_attention_mask = attention_mask.to(device, dtype = torch.float)\n","        output_1 = self.l1(input_ids=ids, attention_mask=mask, token_type_ids=token_type_ids,\n","                                        visual_embeds=visual_feat, visual_token_type_ids=visual_token_type_ids,visual_attention_mask=visual_attention_mask)\n","        hidden_state = output_1.hidden_states[-1]\n","        visual_feat = hidden_state[:, 512:,:]\n","        output = self.pre_classifier(visual_feat)\n","        output = torch.nn.Tanh()(output)\n","        output = self.dropout(output)\n","        output = self.classifier(output)\n","        return output"],"metadata":{"id":"IxXg4S6pSl6Q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Compute metric function definition"],"metadata":{"id":"M32IXvGDTjpn"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"zKDw4L_6SLKW"},"outputs":[],"source":["def compute_metrics(p):\n","    predictions, labels = p\n","    predictions = np.argmax(predictions, axis=2)\n","    # Remove ignored index (special tokens)\n","    true_predictions = [\n","        [label_list[int(p)] for (p, l) in zip(prediction, label) if l != -100]\n","        for prediction, label in zip(predictions, labels)\n","    ]\n","    true_labels = [\n","        [label_list[int(l)] for (p, l) in zip(prediction, label) if l != -100]\n","        for prediction, label in zip(predictions, labels)\n","    ]\n","    results = metric.compute(predictions=true_predictions, references=true_labels)\n","    if return_entity_level_metrics:\n","        # Unpack nested dictionaries\n","        final_results = {}\n","        for key, value in results.items():\n","            if isinstance(value, dict):\n","                for n, v in value.items():\n","                    final_results[f\"{key}_{n}\"] = v\n","            else:\n","                final_results[key] = value\n","        return final_results\n","    else:\n","        return results"]},{"cell_type":"markdown","source":["#Main\n","\n","Load visual precomputed features. Precomputed textual features are not needed because ViasualBERT uses its own tokenizer."],"metadata":{"id":"fYuBivqVTkTw"}},{"cell_type":"code","source":["with open(\"data/FUNSD/dataset/training_data/all_annotations_visual.pickle\", 'rb') as f:\n","    train_data = pickle.load(f)\n","\n","with open(\"data/FUNSD/dataset/testing_data/all_annotations_visual.pickle\", 'rb') as f:\n","    test_data = pickle.load(f)"],"metadata":{"id":"TvI4HornSpFD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Define model, loss and optimizer and create dataset objects"],"metadata":{"id":"YeWaJr-oT6YL"}},{"cell_type":"code","source":["label_dict = {}\n","label_id = 0\n","for doc in train_data:\n","    for i in range(len(train_data[doc][\"form\"])):\n","        if train_data[doc][\"form\"][i][\"label\"] not in label_dict:\n","            label_dict[train_data[doc][\"form\"][i][\"label\"]] = label_id\n","            label_id += 1\n","\n","tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", truncation=True, do_lower_case=True) #Cased\n","model = New_model()\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model.to(device)\n","\n","metric = load_metric(\"seqeval\")\n","return_entity_level_metrics = True\n","label_list = list(label_dict.keys())\n","\n","train_dataset = CustomDataset(doc_info=train_data, tokenizer=tokenizer)\n","test_dataset = CustomDataset(doc_info=test_data, tokenizer=tokenizer)\n","train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n","test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n","\n","optimizer = AdamW(model.parameters(), lr=2e-5)\n","loss_function = torch.nn.CrossEntropyLoss()\n","model.train()\n"],"metadata":{"id":"mB4HnSCgSrZ9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Train and Validation functions\n"],"metadata":{"id":"kmrDeoLoT98p"}},{"cell_type":"code","source":["def train(num_train_epochs):\n","    for _ in range(num_train_epochs):\n","        total_loss = 0\n","        for data in tqdm(train_dataloader):\n","            # get the inputs;\n","            ids = data['ids'].to(device, dtype = torch.long)\n","            mask = data['mask'].to(device, dtype = torch.float)\n","            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n","            labels = data['target'].to(device, dtype = torch.long)\n","            visual_feats = data['visual_feat'].to(device, dtype = torch.float)\n","            object_mask = data['object_mask'].to(device, dtype = torch.float)\n","\n","            optimizer.zero_grad()\n","            outputs = model(ids, mask, token_type_ids,visual_feats,object_mask)\n","            # Change the number of categories\n","            loss = loss_function(outputs.view(-1, 4), labels.view(-1)) #\n","            loss.backward()\n","            total_loss += loss.item()\n","            optimizer.step()\n","        print(\"Train Loss:\", total_loss/len(train_dataloader))"],"metadata":{"id":"s4h0aU1JSt3Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def eval(test_dataloader):\n","    preds_val = None\n","    out_label_ids = None\n","    model.eval()\n","    total_loss = 0\n","    for data in tqdm(test_dataloader):\n","        with torch.no_grad():\n","            # get the inputs;\n","            ids = data['ids'].to(device, dtype = torch.long)\n","            mask = data['mask'].to(device, dtype = torch.float)\n","            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n","            visual_feats = data['visual_feat'].to(device, dtype = torch.float)\n","            object_mask = data['object_mask'].to(device, dtype = torch.float)\n","            labels = data['target'].to(device, dtype = torch.long)\n","\n","            optimizer.zero_grad()\n","            outputs = model(ids, mask, token_type_ids, visual_feats, object_mask)\n","            loss = loss_function(outputs.view(-1, 4), labels.view(-1))\n","            total_loss += loss.item()\n","            if preds_val is None:\n","                preds_val = outputs.detach().cpu().numpy()\n","                out_label_ids = data[\"target\"].detach().cpu().numpy()\n","            else:\n","                preds_val = np.append(preds_val, outputs.detach().cpu().numpy(), axis=0)\n","                out_label_ids = np.append(out_label_ids, data[\"target\"].detach().cpu().numpy(), axis=0)\n","    print(\"Val Loss:\", total_loss/len(test_dataloader))\n","    pprint.pprint(compute_metrics((preds_val, out_label_ids)))\n","    return compute_metrics((preds_val, out_label_ids))\n","\n"],"metadata":{"id":"buulBhuqSyKb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Train Loop\n","\n","Run training for 20 epochs, validating the model every epoch. Everytime a new best model is found, the checkpoint is saved. At the end of trainig loop best performing epoch and associated evaluation results are printed"],"metadata":{"id":"xrXfay5KUBa0"}},{"cell_type":"code","source":["current_f1 = 0\n","best_epoch = 0\n","optimizer = AdamW(model.parameters(), lr=1e-5)\n","\n","for epoch in range(20):\n","    print(\"Epoch:\", epoch+1)\n","    train(1)\n","    val_result = eval(test_dataloader)\n","    if val_result['overall_f1'] > current_f1:\n","        current_f1 = val_result['overall_f1']\n","        best_epoch = epoch\n","        best_val_result = val_result\n","        torch.save(model, 'results/visualbert_funsd.pth')\n","\n","print(\"Best Epoch:\", best_epoch+1)\n","pprint.pprint(best_val_result)"],"metadata":{"id":"GYBoEQSlS0YN"},"execution_count":null,"outputs":[]}]}