{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOnP248+vqEdep2jRdDo6IZ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import torch\n","from torchvision.ops import RoIAlign\n","from torchvision.models import resnet101\n","from torchvision import transforms\n","from PIL import Image\n","from tqdm import tqdm\n","import pickle\n","import argparse\n","import json"],"metadata":{"id":"USrAx1RzN3GO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Feature Extractor Function\n","It employes a visual model, e.g., ResNet101, to extract visual feature from an image and a list of ROIs passed as input arguments."],"metadata":{"id":"8mxwZ0lYOUVp"}},{"cell_type":"code","source":["def extract_roi_features(image_path, rois):\n","    roi_align = RoIAlign(output_size=(1, 1), spatial_scale=1/32, sampling_ratio=-1)\n","    image = Image.open(image_path).convert('RGB')\n","    preprocess = transforms.Compose([\n","        transforms.Resize((800, 800)),  # resize the image to (800, 800) for simplicity\n","        transforms.ToTensor(),\n","        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","    ])\n","    input_tensor = preprocess(image)\n","    input_batch = input_tensor.unsqueeze(0)\n","    input_batch = input_batch.to(device, dtype = torch.float)\n","    with torch.no_grad():\n","        res5_features = resnet(input_batch)\n","    image_indices = torch.zeros((len(rois), 1))\n","    image_indices = image_indices.to(device, dtype = torch.int)\n","    rois = torch.tensor(rois).float()\n","    rois = rois.to(device, dtype = torch.float)\n","    rois = torch.cat((image_indices, rois), dim=1)\n","    pooled_features = roi_align(res5_features, rois)\n","    roi_features = pooled_features.reshape(-1, 2048)\n","    return roi_features.tolist()"],"metadata":{"id":"j2ceFXWFN7C4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Main"],"metadata":{"id":"2RfdzhmrN8xl"}},{"cell_type":"code","source":["parser = argparse.ArgumentParser()\n","parser.add_argument(\"--split\", type=str, default=\"train\", choices=[\"train\", \"test\"])\n","args = parser.parse_args()\n","split = args.split\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","resnet = resnet101(pretrained=True)\n","resnet = torch.nn.Sequential(*list(resnet.children())[:-2])\n","resnet.to(device)\n","resnet.eval()"],"metadata":{"id":"5ueTtTevOLK1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Read entity annotated FUNSD data. They can be downloaded from: https://guillaumejaume.github.io/FUNSD/download/"],"metadata":{"id":"uzyjCC_rO4oa"}},{"cell_type":"code","source":["with open(\"data/FUNSD/dataset/\"+split+\"ing_data/all_annotations.json\", 'r') as f:\n","    data = json.load(f)\n","\n","with open(\"data/FUNSD/dataset/\"+split+\"ing_data/all_annotations.json\", 'r') as f:\n","    copy = json.load(f)"],"metadata":{"id":"nZV3zOTAOQEY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Apply feature extractor on th entire dataset and a field \"visual_list\" to the original json file"],"metadata":{"id":"DZNTfkUrO-Ck"}},{"cell_type":"code","source":["for key in tqdm(data.keys()):\n","    image_path = \"data/FUNSD/dataset/\"+split+\"ing_data/images/\"+key+\".png\"\n","    doc = data[key][\"form\"]\n","    object_list = []\n","    for i in range(len(doc)):\n","        object_list.append(doc[i][\"box\"])\n","        if len(doc[i][\"box\"]) == 0:\n","            continue\n","    features = extract_roi_features(image_path, object_list)\n","    copy[key][\"visual_list\"] = features"],"metadata":{"id":"B7V5bqLVORfe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Save the new data structure containing the computed visual features as pickle file."],"metadata":{"id":"Pxq5aLQCPLLV"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"cGMkiPSvNwCd"},"outputs":[],"source":["with open('data/FUNSD/dataset/'+split+'ing_data/all_annotations_visual.pickle', 'wb') as f:\n","    pickle.dump(copy, f, protocol=pickle.HIGHEST_PROTOCOL)"]}]}