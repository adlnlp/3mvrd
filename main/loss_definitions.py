# -*- coding: utf-8 -*-
"""loss_definitions.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nC_XC9Nkx5dxJBtDVyQkXfaQxmxFpeFx
"""

import torch.nn.functional as F
import torch.nn as nn
def avg_score(te_scores_Tensor):
#     print(te_scores_Tensor.size())
    mean_Tensor = torch.mean(te_scores_Tensor, dim=1)
#     print(mean_Tensor)
    return mean_Tensor
def cos_sim_loss(teacher_logits_list, student_logits):
    loss = 0
    for teacher_logits in teacher_logits_list:
        loss += F.cosine_similarity(teacher_logits,student_logits).mean()
    return -loss

def distillation_loss(y, labels, logits, T, alpha=0.7):

    print(nn.KLDivLoss(reduction="batchmean")(F.log_softmax(y/T,dim=1), logits))
    return nn.KLDivLoss(reduction="batchmean")(F.log_softmax(y/T,dim=1), logits) * (T*T * 2.0 * alpha) + F.cross_entropy(y, labels) * (1. - alpha)

def triplet_distance(s_logits, t_logits):
    return (s_logits - t_logits).pow(2).sum()

def distillation_loss(s_logits, labels, t_logits, T=2.0, alpha=0.7):
    print(s_logits.shape)
    #s_logits = s_logits - s_logits.min(1, keepdim=True)[0]  # Shift to make all elements non-negative
    print(nn.KLDivLoss(reduction="batchmean")(F.log_softmax(s_logits/T,dim=1), t_logits))
    s_logits = F.softmax(s_logits, dim=1)  # Optional: Normalize to make it a proper probability distribution

    #print((F.log_softmax(s_logits/T,dim=1).shape))
    return nn.KLDivLoss(reduction="batchmean")(F.softmax(s_logits/T,dim=1), F.softmax(t_logits)) * (T*T * 2.0 * alpha) + F.cross_entropy(s_logits, labels) * (1. - alpha)

def triplet_distance(t1, t2):
    return (t1 - t2).pow(2).sum()

import torch
def triplet_distance(t1, t2):
    # Calculate squared distance for batches (number_objects, hidden_states)
    return (t1 - t2).pow(2).sum(dim=1)  # Sum over hidden_states, keep batch dimension

def calculate_triplet_margin_loss(anchor, tensor1, tensor2, margin=1.0):
    # Calculate distances for each pair in the batch
    distance1 = triplet_distance(anchor, tensor1)
    distance2 = triplet_distance(anchor, tensor2)
    # Determine positive and negative based on distances
    positive_distances = torch.where(distance1 < distance2, distance1, distance2)
    negative_distances = torch.where(distance1 >= distance2, distance1, distance2)

    # Calculate triplet margin loss for each triplet in the batch
    losses = torch.clamp(positive_distances - negative_distances + margin, min=0)

    # Return the average loss across all triplets
    return losses.mean()

def teacher_pred_loss(teacher_logits_list, student_logits):
    loss = 0
    for teacher_logits in teacher_logits_list:
        loss += F.cross_entropy(student_logits, torch.argmax(teacher_logits,dim=-1))
    return loss

def teacher_pred_regression_loss(teacher_logits_list, student_logits):
    loss = 0
    for teacher_logits in teacher_logits_list:
        loss += F.cross_entropy(student_logits, F.softmax(teacher_logits,dim=-1))
    return loss

def triplet_distance_distill(s_logits, t_logits):
    return (s_logits - t_logits).pow(2).sum()/s_logits.shape[0]