# -*- coding: utf-8 -*-
"""train.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wq9lXEWjbGJ_YUPm1e57JKf3yc5CEFYl
"""

def train(num_train_epochs,loss_types = loss_type):
    for _ in range(num_train_epochs):
        total_loss = 0
        for data in tqdm(train_dataloader):
            # token level inputs
            token_inputs = data['token_encoding']
            input_ids = token_inputs['input_ids'].to(device, dtype = torch.long)
            bbox = token_inputs['bbox'].to(device, dtype = torch.long)
            pixel_values = token_inputs['pixel_values'].to(device, dtype = torch.float)
            token_labels = token_inputs['labels'].to(device, dtype = torch.long)
            token_attention_mask = token_inputs['attention_mask'].to(device, dtype = torch.float)
            token_type_ids = token_inputs['token_type_ids'].to(device, dtype = torch.long)
            objt_ids = data['objt_ids'].to(device, dtype = torch.long)

            # get the visualbert inputs
            visualbert_ids = data['visualbert_ids'].to(device, dtype = torch.long)
            visualbert_mask = data['visualbert_mask'].to(device, dtype = torch.float)
            visualbert_token_type_ids = data['visualbert_token_type_ids'].to(device, dtype = torch.long)

            # get the lxmert inputs
            lxmert_ids = data['lxmert_ids'].to(device, dtype = torch.long)
            lxmert_mask = data['lxmert_mask'].to(device, dtype = torch.float)
            lxmert_token_type_ids = data['lxmert_token_type_ids'].to(device, dtype = torch.long)

            # entity representations
            entity_labels = data['target'].to(device, dtype = torch.long)
            visual_feats = data['visual_feat'].to(device, dtype = torch.float)
            bert_cls = data['bert_cls'].to(device, dtype = torch.float)
            entity_attention_mask = data['object_mask'].to(device, dtype = torch.float)
            norm_bbox = data['norm_bbox'].to(device, dtype = torch.float)
            positional_encoding = data['positional_encoding'].to(device, dtype = torch.float)
            loss_list = []


            optimizer.zero_grad()
            output_dict = model(visualbert_ids, visualbert_mask, visualbert_token_type_ids,
                lxmert_ids, lxmert_mask, lxmert_token_type_ids,
                input_ids, bbox, pixel_values, token_attention_mask,token_type_ids,
                visual_feats,bert_cls,entity_attention_mask,norm_bbox,positional_encoding)
            # Change the number of categories
            #### Warm Training
            warm_loss = loss_function(output_dict['token_entity_logits'].view(-1, 100), objt_ids.view(-1))
            # ***************************Student Cross Entropy Loss*****************************
            # Entity Level Cross-Entropy Loss
            entity_s_logits = output_dict['std_entity_logits']
            ce_entity_loss = loss_function(entity_s_logits.view(-1, 7), entity_labels.view(-1)) #
            if 'entity_student_ce' in loss_types:
              loss_list.append(ce_entity_loss)
            token_s_logits = output_dict['std_token_logits']
            ce_token_loss = loss_function(token_s_logits.view(-1, 15), token_labels.view(-1))
            # Token Level Cross-Entropy Loss
            if 'token_student_ce' in loss_types:
              loss_list.append(ce_token_loss/5)

            token_labels = token_labels.flatten()
            token_t1_logits = output_dict['token_t1_logits'].reshape(-1, 15)
            token_t2_logits = output_dict['token_t2_logits'].reshape(-1, 15)
            flatten_token_s_logits = token_s_logits.reshape(-1, 15)

            mask = token_labels != -100
            filter_token_t1_logits = token_t1_logits[mask]
            filter_token_t2_logits = token_t2_logits[mask]
            filter_flatten_token_s_logits = flatten_token_s_logits[mask]
            filter_token_labels = token_labels[mask]

            entity_labels = entity_labels.flatten()
            entity_t1_logits = output_dict['entity_t1_logits'].reshape(-1, 7)
            entity_t2_logits = output_dict['entity_t2_logits'].reshape(-1, 7)
            flatten_entity_s_logits = output_dict['std_entity_logits'].reshape(-1, 7)

            mask = entity_labels != -100
            filter_entity_t1_logits = entity_t1_logits[mask]
            filter_entity_t2_logits = entity_t2_logits[mask]
            filter_flatten_entity_s_logits = flatten_entity_s_logits[mask]
            filter_entity_labels = entity_labels[mask]

            # Similarity Loss
            if 'token_similarity' in loss_types:
              token_teacher_logits = [filter_token_t1_logits,filter_token_t2_logits]
              token_sim_loss = cos_sim_loss(token_teacher_logits,filter_flatten_token_s_logits)
              loss_list.append(token_sim_loss)

            if 'entity_similarity' in loss_types:
              entity_teacher_logits = [filter_entity_t1_logits,filter_entity_t2_logits]
              entity_sim_loss = cos_sim_loss(entity_teacher_logits,filter_flatten_entity_s_logits)
              loss_list.append(entity_sim_loss)

            #### Token Knowledge Distilling Loss
            # print(output_dict['token_t1_logits'].shape)
            # print(token_labels.shape)
            if 'token_distilling' in loss_types:
              te_token_scores = torch.stack([filter_token_t1_logits,filter_token_t2_logits], dim=1)
              token_ave_logit = avg_score(te_token_scores)
              token_distill_loss = triplet_distance_distill(filter_flatten_token_s_logits,token_ave_logit)
              loss_list.append(token_distill_loss/5)

            #token_distill_loss = distillation_loss(filter_flatten_token_s_logits, filter_token_labels,token_ave_logit)
            if 'entity_distilling' in loss_types:
              te_entity_scores = torch.stack([filter_entity_t1_logits,filter_entity_t2_logits], dim=1)
              entity_ave_logit = avg_score(te_entity_scores)
              entity_distill_loss = triplet_distance_distill(filter_flatten_entity_s_logits,entity_ave_logit)
              loss_list.append(entity_distill_loss)

            #### teacher_pred_loss

            if 'token_teacher_pre' in loss_types:
              token_teacher_logits = [filter_token_t1_logits,filter_token_t2_logits]
              token_pre_loss = teacher_pred_loss(token_teacher_logits,filter_flatten_token_s_logits)
              loss_list.append(token_pre_loss)

            if 'entity_similarity' in loss_types:
              entity_teacher_logits = [filter_entity_t1_logits,filter_entity_t2_logits]
              entity_pre_loss = teacher_pred_loss(entity_teacher_logits,filter_flatten_entity_s_logits)
              loss_list.append(entity_pre_loss)


            #### teacher_pred_loss
            if 'token_teach_reg' in loss_types:
              token_teacher_logits = [filter_token_t1_logits,filter_token_t2_logits]
              token_reg_loss = teacher_pred_regression_loss(token_teacher_logits,filter_flatten_token_s_logits)
              loss_list.append(token_reg_loss)

            if 'entity_teach_reg' in loss_types:
              entity_teacher_logits = [filter_entity_t1_logits,filter_entity_t2_logits]
              entity_reg_loss = teacher_pred_regression_loss(entity_teacher_logits,filter_flatten_entity_s_logits)
              loss_list.append(entity_reg_loss)

            #### Cross-grained Loss
            token_objt_ids = objt_ids.flatten()
            flatten_token_s_hidden = output_dict['std_token_hidden'].reshape(-1, 768)
            flatten_token_t1_hidden = output_dict['token_t1_hidden'].reshape(-1, 768)
            flatten_token_t2_hidden = output_dict['token_t2_hidden'].reshape(-1, 768)

            flatten_entity_s_hidden = output_dict['std_entity_hidden'].reshape(-1, 768)
            flatten_entity_t1_hidden = output_dict['entity_t1_hidden'].reshape(-1, 768)
            flatten_entity_t2_hidden = output_dict['entity_t2_hidden'].reshape(-1, 768)

            mask = token_objt_ids != -100
            filter_flatten_token_s_hidden = flatten_token_s_hidden[mask]
            filter_flatten_token_t1_hidden = flatten_token_t1_hidden[mask]
            filter_flatten_token_t2_hidden = flatten_token_t2_hidden[mask]
            filter_token_objt_ids = token_objt_ids[mask]

            filter_flatten_entity_s_hidden = flatten_entity_s_hidden[filter_token_objt_ids]
            filter_flatten_entity_t1_hidden = flatten_entity_t1_hidden[filter_token_objt_ids]
            filter_flatten_entity_t2_hidden = flatten_entity_t2_hidden[filter_token_objt_ids]


            ## Fine->Coarse Grained
            if 'token_triplets' in loss_types:
              fine_coarse_loss = calculate_triplet_margin_loss(filter_flatten_entity_s_hidden,filter_flatten_token_t1_hidden,filter_flatten_token_t2_hidden)
              loss_list.append(fine_coarse_loss)

            if 'entity_triplets' in loss_types:
              coarse_fine_loss = calculate_triplet_margin_loss(filter_flatten_token_s_hidden,filter_flatten_entity_t1_hidden,filter_flatten_entity_t2_hidden)
              loss_list.append(coarse_fine_loss)
            #### Final Loss
            loss = torch.sum(torch.stack(loss_list))+0.2*warm_loss
            loss.backward()
            total_loss += loss.item()
            optimizer.step()
        print("Train Loss:", total_loss/len(train_dataloader))